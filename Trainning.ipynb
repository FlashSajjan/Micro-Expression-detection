{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8d145d6-1683-4629-8e03-8ca29b948ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Flatten, Dense, Dropout, concatenate, add\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38f67371-be36-4a83-b1f1-c91d2ac4080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenCV Haar Cascade classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42d9f4db-72e9-4379-8eb1-94e0ef15f244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel columns: Index(['Subject', 'Filename', 'Unnamed: 2', 'OnsetF', 'ApexF1', 'ApexF2',\n",
      "       'OffsetF', 'Unnamed: 7', 'Onset', 'Total', 'AU', 'Emotion'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Data Loading and Preprocessing from Excel\n",
    "\n",
    "base_dataset_path = \"E:/FacialMicroExpression/data\"\n",
    "excel_file_path = \"Section A.xls\"\n",
    "output_size = (112, 112)\n",
    "motion_threshold = 1e-3\n",
    "\n",
    "# Read the Excel file to get the emotion labels for each sub folder\n",
    "def load_data_from_excel(excel_file_path):\n",
    "    \"\"\"Reads the Excel file containing filenames and labels.\"\"\"\n",
    "    data = pd.read_excel(excel_file_path)\n",
    "    return data\n",
    "print(\"Excel columns:\", load_data_from_excel(excel_file_path).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb78cf82-961c-437d-81a0-f7692f6e2eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   Subject Filename  Unnamed: 2  OnsetF  ApexF1 ApexF2 OffsetF  Unnamed: 7  \\\n",
      "0        1  EP01_12         NaN      73      81      \\      91         NaN   \n",
      "1        1  EP01_12         NaN     163     169      \\     177         NaN   \n",
      "2        1   EP01_5         NaN     113     121    125     133         NaN   \n",
      "3        1   EP01_8         NaN      67      75      \\      81         NaN   \n",
      "4        1   EP03_1         NaN      79      91     95     105         NaN   \n",
      "\n",
      "        Onset       Total  AU     Emotion  \n",
      "0  150.000000  316.666667   4       tense  \n",
      "1  116.666667         250   4       tense  \n",
      "2  150.000000         350  12   happiness  \n",
      "3  150.000000         250  14  repression  \n",
      "4  216.666667         450  17  repression  \n",
      "\n",
      "Cleaned Data:\n",
      "  Subject Filename  Unnamed: 2  OnsetF  ApexF1 ApexF2 OffsetF  Unnamed: 7  \\\n",
      "0       1  ep01_12         NaN      73      81      \\      91         NaN   \n",
      "1       1  ep01_12         NaN     163     169      \\     177         NaN   \n",
      "2       1   ep01_5         NaN     113     121    125     133         NaN   \n",
      "3       1   ep01_8         NaN      67      75      \\      81         NaN   \n",
      "4       1   ep03_1         NaN      79      91     95     105         NaN   \n",
      "\n",
      "        Onset       Total  AU     Emotion  \n",
      "0  150.000000  316.666667   4       tense  \n",
      "1  116.666667         250   4       tense  \n",
      "2  150.000000         350  12   happiness  \n",
      "3  150.000000         250  14  repression  \n",
      "4  216.666667         450  17  repression  \n",
      "\n",
      "Unique Emotions:\n",
      "['tense' 'happiness' 'repression' 'disgust' 'surprise' 'comtempt' 'fear'\n",
      " 'sadness']\n",
      "\n",
      "Unique Subjects:\n",
      "19\n",
      "\n",
      "Unique Filenames:\n",
      "120\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Filename</th>\n",
       "      <th>OnsetF</th>\n",
       "      <th>ApexF1</th>\n",
       "      <th>ApexF2</th>\n",
       "      <th>OffsetF</th>\n",
       "      <th>Onset</th>\n",
       "      <th>Total</th>\n",
       "      <th>AU</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ep01_12</td>\n",
       "      <td>73</td>\n",
       "      <td>81</td>\n",
       "      <td>\\</td>\n",
       "      <td>91</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>316.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>tense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ep01_12</td>\n",
       "      <td>163</td>\n",
       "      <td>169</td>\n",
       "      <td>\\</td>\n",
       "      <td>177</td>\n",
       "      <td>116.666667</td>\n",
       "      <td>250</td>\n",
       "      <td>4</td>\n",
       "      <td>tense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ep01_5</td>\n",
       "      <td>113</td>\n",
       "      <td>121</td>\n",
       "      <td>125</td>\n",
       "      <td>133</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>350</td>\n",
       "      <td>12</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>ep01_8</td>\n",
       "      <td>67</td>\n",
       "      <td>75</td>\n",
       "      <td>\\</td>\n",
       "      <td>81</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>250</td>\n",
       "      <td>14</td>\n",
       "      <td>repression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ep03_1</td>\n",
       "      <td>79</td>\n",
       "      <td>91</td>\n",
       "      <td>95</td>\n",
       "      <td>105</td>\n",
       "      <td>216.666667</td>\n",
       "      <td>450</td>\n",
       "      <td>17</td>\n",
       "      <td>repression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Subject Filename  OnsetF  ApexF1 ApexF2 OffsetF       Onset       Total  AU  \\\n",
       "0       1  ep01_12      73      81      \\      91  150.000000  316.666667   4   \n",
       "1       1  ep01_12     163     169      \\     177  116.666667         250   4   \n",
       "2       1   ep01_5     113     121    125     133  150.000000         350  12   \n",
       "3       1   ep01_8      67      75      \\      81  150.000000         250  14   \n",
       "4       1   ep03_1      79      91     95     105  216.666667         450  17   \n",
       "\n",
       "      Emotion  \n",
       "0       tense  \n",
       "1       tense  \n",
       "2   happiness  \n",
       "3  repression  \n",
       "4  repression  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data cleaning and normalization\n",
    "# Path to the Excel file\n",
    "excel_file_path = \"Section A.xls\"\n",
    "\n",
    "# Load the Excel file\n",
    "data = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Display the original DataFrame for reference\n",
    "print(\"Original Data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Data cleaning and normalization\n",
    "# Normalize columns: Subject, Filename, and Emotion\n",
    "data['Subject'] = data['Subject'].astype(str).str.strip().str.lower()  # Normalize Subject column\n",
    "data['Filename'] = data['Filename'].astype(str).str.strip().str.lower()  # Normalize Filename column\n",
    "data['Emotion'] = data['Emotion'].astype(str).str.strip().str.lower()  # Normalize Emotion column\n",
    "\n",
    "# Handle missing values in critical columns\n",
    "data.dropna(subset=['Subject', 'Filename', 'Emotion'], inplace=True)  # Drop rows with missing values in these columns\n",
    "\n",
    "# Remove duplicates if any (considering all columns for uniqueness)\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(\"\\nCleaned Data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Display unique emotions for verification\n",
    "print(\"\\nUnique Emotions:\")\n",
    "print(data['Emotion'].unique())\n",
    "\n",
    "# Display the count of unique subjects and filenames for debugging\n",
    "print(\"\\nUnique Subjects:\")\n",
    "print(data['Subject'].nunique())\n",
    "print(\"\\nUnique Filenames:\")\n",
    "print(data['Filename'].nunique())\n",
    "data = data.drop(columns=['Unnamed: 2', 'Unnamed: 7'], errors='ignore')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b09de89-4cfb-41aa-963d-bd52ae79731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_EP_folders(base_dataset_path, data):\n",
    "    image_data = []\n",
    "    image_labels = []\n",
    "    unmatched_folders = []\n",
    "\n",
    "    for subfolder in os.listdir(base_dataset_path):\n",
    "        subfolder_path = os.path.join(base_dataset_path, subfolder)\n",
    "\n",
    "        # Format the subject (e.g., 1 -> sub01, 2 -> sub02)\n",
    "        subject_id = subfolder.strip().lower().replace(\"sub\", \"\")  # Remove 'sub' and match\n",
    "        matching_subjects = data[data['Subject'].astype(str).str.zfill(2) == subject_id]\n",
    "\n",
    "        if not matching_subjects.empty:\n",
    "            for ep_folder in os.listdir(subfolder_path):\n",
    "                ep_folder_path = os.path.join(subfolder_path, ep_folder)\n",
    "\n",
    "                # Match ep_folder with Filename column in Excel (normalize case)\n",
    "                matching_filenames = matching_subjects[matching_subjects['Filename'].str.strip().str.lower() == ep_folder.strip().lower()]\n",
    "\n",
    "                if not matching_filenames.empty and os.path.isdir(ep_folder_path):\n",
    "                    for file_name in os.listdir(ep_folder_path):\n",
    "                        if file_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                            image_path = os.path.join(ep_folder_path, file_name)\n",
    "\n",
    "                            image = cv2.imread(image_path)\n",
    "                            if image is not None:\n",
    "                                resized_image = cv2.resize(image, (112, 112))\n",
    "                                image_data.append(resized_image)\n",
    "\n",
    "                                # Extract emotion label from matching row in Excel\n",
    "                                label = matching_filenames['Emotion'].values[0]\n",
    "                                image_labels.append(label)\n",
    "                else:\n",
    "                    unmatched_folders.append(ep_folder)\n",
    "        else:\n",
    "            unmatched_folders.append(subfolder)\n",
    "\n",
    "    print(f\"Loaded {len(image_data)} images.\")\n",
    "    print(f\"Loaded {len(image_labels)} labels.\")\n",
    "    print(f\"Unique labels: {set(image_labels)}\")\n",
    "    print(f\"Unmatched folders: {set(unmatched_folders)}\")\n",
    "\n",
    "    return image_data, image_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "344cabb2-4f00-49f4-925b-42d1e268d920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 33217 images.\n",
      "Loaded 33217 labels.\n",
      "Unique labels: {'disgust', 'comtempt', 'fear', 'sadness', 'repression', 'surprise', 'happiness', 'tense'}\n",
      "Unmatched folders: {'EP13_7.avi', 'EP14_1.avi', 'EP08_8.avi', 'EP07_3.avi', 'EP09_4.avi', 'EP04_2.avi', 'EP02_21.avi', 'EP12_4.avi', 'EP07_9.avi', 'EP13_4.avi', 'EP09_2.avi', 'EP07_1', 'EP01_4.avi', 'EP07_4.avi', 'EP13_5.avi', 'EP12_11_1.avi', 'EP01_12.avi', 'EP01_5.avi', 'EP07_3', 'EP17_9.avi', 'EP12_2_2.avi', 'EP17_1_2.avi', 'EP12_4_5.avi', 'EP02_19.avi', 'EP01_6.avi', 'EP02_8.avi', 'EP04.avi', 'EP07_6.avi', 'EP13_3.avi', 'EP17_1_4.avi', 'EP17_2.avi', 'EP15_3.avi', 'EP03_2.avi', 'EP08_1.avi', 'EP02_10.avi', 'EP17.avi', 'EP12_3.avi', 'EP07_1.avi', 'EP13_8.avi', 'EP03_1', 'EP06_7.avi', 'EP12_4_10.avi', 'EP10_3.avi', 'EP08_5.avi', 'EP14_8.avi', 'EP12_1.avi', 'EP12_2_3.avi', 'EP17_3.avi', 'EP17_8.avi', 'EP08_4.avi', 'EP13_2', 'EP02_6.avi', 'EP17_2', 'EP17_1.avi', 'EP15_1.avi', 'EP09_2', 'EP13_6.avi', 'EP12_2_7.avi', 'EP17_4.avi', 'EP12_4_1.avi', 'EP08_6.avi', 'EP17_1_3.avi', 'EP05_4.avi', 'EP12_2_9.avi', 'EP12_4_9.avi', 'EP02_5.avi', 'EP08_2.avi', 'EP12.avi', 'EP06_1', 'EP17_1', 'EP03_2', 'EP01_8.avi', 'EP09_1', 'EP05_2.avi', 'EP13_2.avi', 'EP16_3.avi', 'EP12_2_5.avi', 'EP12_4_4.avi', 'EP14_8', 'EP17_7', 'EP12_4_2.avi', 'EP09_7.avi', 'EP03_5.avi', 'EP02_22.avi', 'EP12_4_6.avi', 'EP07.avi', 'EP16_2.avi', 'EP17_7.avi', 'EP12_2_8.avi', 'EP12_2.avi', 'EP07_10.avi', 'EP12_9.avi', 'EP17_1_1.avi', 'EP13_1', 'EP07_11.avi', 'EP12_2_12.avi', 'EP11_1.avi', 'hs_err_pid8508.log', 'EP02_4.avi', 'EP08_9.avi', 'EP03_4.avi', 'EP04_5', 'EP12_2_11.avi', 'EP15_2.avi', 'EP03_1.avi', 'EP06_1.avi', 'EP12_2_10.avi', 'EP09_6.avi', 'EP07_7.avi', 'EP09_1.avi', 'EP10.avi', 'EP13_1.avi', 'EP12_2_1.avi', 'EP01_1.avi', 'EP07_2.avi', 'EP09_3.avi', 'EP12_2_4.avi', 'EP12_4_3.avi', 'EP17_10.avi', 'EP02_2.avi', 'EP07_8.avi', 'EP17_3', 'EP09_8.avi', 'EP12_4_7.avi', 'EP06_2.avi', 'EP17_6.avi', 'EP12_11_2.avi', 'EP05_3.avi', 'EP16_1.avi', 'EP11_2.avi', 'EP09_5.avi', 'EP08_3.avi', 'EP12_4_8.avi', 'EP11.avi'}\n",
      "Image 1 shape: (112, 112, 3)\n",
      "Image 2 shape: (112, 112, 3)\n",
      "Image 3 shape: (112, 112, 3)\n",
      "Image 4 shape: (112, 112, 3)\n",
      "Image 5 shape: (112, 112, 3)\n",
      "Unique labels in the dataset: ['tense' 'happiness' 'repression' 'disgust' 'surprise' 'comtempt' 'fear'\n",
      " 'sadness']\n"
     ]
    }
   ],
   "source": [
    "image_data, image_labels = load_images_from_EP_folders(base_dataset_path, data)\n",
    "for idx, image in enumerate(image_data[:5]):  # Check first 5 images\n",
    "    print(f\"Image {idx + 1} shape: {image.shape}\")\n",
    "unique_labels = data['Emotion'].unique()\n",
    "print(f\"Unique labels in the dataset: {unique_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83105db5-fa2c-475a-9400-1f4f64dc1421",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1: Label = tense\n",
      "Image 2: Label = tense\n",
      "Image 3: Label = tense\n",
      "Image 4: Label = tense\n",
      "Image 5: Label = tense\n"
     ]
    }
   ],
   "source": [
    "for i, (image, label) in enumerate(zip(image_data[:5], image_labels[:5])):  # First 5 pairs\n",
    "    print(f\"Image {i + 1}: Label = {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af1848e7-6760-4a10-84ee-9efc989aa056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dynamic_image(frames, normalized=True):\n",
    "    \"\"\" Takes a list of frames and returns either a raw or normalized dynamic image.\"\"\"\n",
    "    num_channels = frames[0].shape[2]\n",
    "    channel_frames = _get_channel_frames(frames, num_channels)\n",
    "    channel_dynamic_images = [_compute_dynamic_image(channel) for channel in channel_frames]\n",
    "\n",
    "    dynamic_image = cv2.merge(tuple(channel_dynamic_images))\n",
    "    if normalized:\n",
    "        dynamic_image = cv2.normalize(dynamic_image, None, 0, 255, norm_type=cv2.NORM_MINMAX)\n",
    "        dynamic_image = dynamic_image.astype('uint8')\n",
    "\n",
    "    return dynamic_image\n",
    "\n",
    "\n",
    "def _get_channel_frames(iter_frames, num_channels):\n",
    "    \"\"\" Takes a list of frames and returns a list of frame lists split by channel. \"\"\"\n",
    "    frames = [[] for channel in range(num_channels)]\n",
    "\n",
    "    for frame in iter_frames:\n",
    "        for channel_frames, channel in zip(frames, cv2.split(frame)):\n",
    "            channel_frames.append(channel.reshape((*channel.shape[0:2], 1)))\n",
    "    for i in range(len(frames)):\n",
    "        frames[i] = np.array(frames[i])\n",
    "    return frames\n",
    "\n",
    "\n",
    "def _compute_dynamic_image(frames):\n",
    "    num_frames, h, w, depth = frames.shape\n",
    "\n",
    "    y = np.zeros((num_frames, h, w, depth))\n",
    "\n",
    "    ids = np.ones(num_frames)\n",
    "\n",
    "    fw = np.zeros(num_frames)\n",
    "    for n in range(num_frames):\n",
    "        cumulative_indices = np.array(range(n, num_frames)) + 1\n",
    "        fw[n] = np.sum(((2*cumulative_indices) - num_frames) / cumulative_indices)\n",
    "\n",
    "    for v in range(int(np.max(ids))):\n",
    "        indv = np.array(np.where(ids == v+1))\n",
    "\n",
    "        a1 = frames[indv, :, :, :]\n",
    "        a2 = np.reshape(fw, (indv.shape[1], 1, 1, 1))\n",
    "        a3 = a1 * a2\n",
    "        print(indv.shape[1])\n",
    "\n",
    "        y = np.sum(a3[0], axis=0)\n",
    "        print(y.shape)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "306184c6-acd4-410e-919e-0a8714e33ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_videos(base_dataset_path, data, output_size=(112, 112)):\n",
    "    video_data = []\n",
    "    video_labels = []\n",
    "\n",
    "    # Iterate through subject folders (sub01, sub02, ...)\n",
    "    for subject_folder in os.listdir(base_dataset_path):\n",
    "        subject_path = os.path.join(base_dataset_path, subject_folder)\n",
    "\n",
    "        # Check if it's a valid directory (e.g., sub01, sub02)\n",
    "        if os.path.isdir(subject_path):\n",
    "            # Match subject with Excel data (e.g., sub01 with Subject 1)\n",
    "            matching_subjects = data[\n",
    "                data['Subject'].astype(str).str.strip().str.lower() == subject_folder.strip().lower().replace('sub', '')\n",
    "            ]\n",
    "            label = matching_subjects['Emotion'].values[0] if not matching_subjects.empty else None\n",
    "\n",
    "            # Process video files inside the subject folder\n",
    "            for file_name in os.listdir(subject_path):\n",
    "                if file_name.endswith('.avi'):  # Look for .avi video files\n",
    "                    video_filename = file_name.split('.')[0]  # Get EPXXXX part of the filename (e.g., EP01)\n",
    "\n",
    "                    # Match video filename with Excel 'Filename' column\n",
    "                    matching_video = matching_subjects[matching_subjects['Filename'].str.strip().str.lower() == video_filename.strip().lower()]\n",
    "                    if not matching_video.empty:\n",
    "                        video_path = os.path.join(subject_path, file_name)\n",
    "                        cap = cv2.VideoCapture(video_path)\n",
    "                        frames = []\n",
    "\n",
    "                        # Extract frames from the video\n",
    "                        while cap.isOpened():\n",
    "                            ret, frame = cap.read()\n",
    "                            if not ret:\n",
    "                                break\n",
    "                            resized_frame = cv2.resize(frame, output_size)  # Resize the frame\n",
    "                            frames.append(resized_frame)\n",
    "                        cap.release()\n",
    "\n",
    "                        # Generate dynamic image from frames (optional)\n",
    "                        if len(frames) > 0:\n",
    "                            dynamic_image = get_dynamic_image(frames)\n",
    "                            video_data.append(dynamic_image)\n",
    "                            video_labels.append(label)\n",
    "\n",
    "    return video_data, video_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04f4ffe6-070d-4a96-aeca-a45ea9a1e1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "(112, 112, 1)\n",
      "168\n",
      "(112, 112, 1)\n",
      "168\n",
      "(112, 112, 1)\n",
      "94\n",
      "(112, 112, 1)\n",
      "94\n",
      "(112, 112, 1)\n",
      "94\n",
      "(112, 112, 1)\n",
      "93\n",
      "(112, 112, 1)\n",
      "93\n",
      "(112, 112, 1)\n",
      "93\n",
      "(112, 112, 1)\n",
      "76\n",
      "(112, 112, 1)\n",
      "76\n",
      "(112, 112, 1)\n",
      "76\n",
      "(112, 112, 1)\n",
      "141\n",
      "(112, 112, 1)\n",
      "141\n",
      "(112, 112, 1)\n",
      "141\n",
      "(112, 112, 1)\n",
      "172\n",
      "(112, 112, 1)\n",
      "172\n",
      "(112, 112, 1)\n",
      "172\n",
      "(112, 112, 1)\n",
      "131\n",
      "(112, 112, 1)\n",
      "131\n",
      "(112, 112, 1)\n",
      "131\n",
      "(112, 112, 1)\n",
      "204\n",
      "(112, 112, 1)\n",
      "204\n",
      "(112, 112, 1)\n",
      "204\n",
      "(112, 112, 1)\n",
      "141\n",
      "(112, 112, 1)\n",
      "141\n",
      "(112, 112, 1)\n",
      "141\n",
      "(112, 112, 1)\n",
      "178\n",
      "(112, 112, 1)\n",
      "178\n",
      "(112, 112, 1)\n",
      "178\n",
      "(112, 112, 1)\n",
      "295\n",
      "(112, 112, 1)\n",
      "295\n",
      "(112, 112, 1)\n",
      "295\n",
      "(112, 112, 1)\n",
      "255\n",
      "(112, 112, 1)\n",
      "255\n",
      "(112, 112, 1)\n",
      "255\n",
      "(112, 112, 1)\n",
      "128\n",
      "(112, 112, 1)\n",
      "128\n",
      "(112, 112, 1)\n",
      "128\n",
      "(112, 112, 1)\n",
      "199\n",
      "(112, 112, 1)\n",
      "199\n",
      "(112, 112, 1)\n",
      "199\n",
      "(112, 112, 1)\n",
      "348\n",
      "(112, 112, 1)\n",
      "348\n",
      "(112, 112, 1)\n",
      "348\n",
      "(112, 112, 1)\n",
      "174\n",
      "(112, 112, 1)\n",
      "174\n",
      "(112, 112, 1)\n",
      "174\n",
      "(112, 112, 1)\n",
      "164\n",
      "(112, 112, 1)\n",
      "164\n",
      "(112, 112, 1)\n",
      "164\n",
      "(112, 112, 1)\n",
      "302\n",
      "(112, 112, 1)\n",
      "302\n",
      "(112, 112, 1)\n",
      "302\n",
      "(112, 112, 1)\n",
      "447\n",
      "(112, 112, 1)\n",
      "447\n",
      "(112, 112, 1)\n",
      "447\n",
      "(112, 112, 1)\n",
      "394\n",
      "(112, 112, 1)\n",
      "394\n",
      "(112, 112, 1)\n",
      "394\n",
      "(112, 112, 1)\n",
      "171\n",
      "(112, 112, 1)\n",
      "171\n",
      "(112, 112, 1)\n",
      "171\n",
      "(112, 112, 1)\n",
      "230\n",
      "(112, 112, 1)\n",
      "230\n",
      "(112, 112, 1)\n",
      "230\n",
      "(112, 112, 1)\n",
      "172\n",
      "(112, 112, 1)\n",
      "172\n",
      "(112, 112, 1)\n",
      "172\n",
      "(112, 112, 1)\n",
      "157\n",
      "(112, 112, 1)\n",
      "157\n",
      "(112, 112, 1)\n",
      "157\n",
      "(112, 112, 1)\n",
      "186\n",
      "(112, 112, 1)\n",
      "186\n",
      "(112, 112, 1)\n",
      "186\n",
      "(112, 112, 1)\n",
      "155\n",
      "(112, 112, 1)\n",
      "155\n",
      "(112, 112, 1)\n",
      "155\n",
      "(112, 112, 1)\n",
      "185\n",
      "(112, 112, 1)\n",
      "185\n",
      "(112, 112, 1)\n",
      "185\n",
      "(112, 112, 1)\n",
      "227\n",
      "(112, 112, 1)\n",
      "227\n",
      "(112, 112, 1)\n",
      "227\n",
      "(112, 112, 1)\n",
      "201\n",
      "(112, 112, 1)\n",
      "201\n",
      "(112, 112, 1)\n",
      "201\n",
      "(112, 112, 1)\n",
      "373\n",
      "(112, 112, 1)\n",
      "373\n",
      "(112, 112, 1)\n",
      "373\n",
      "(112, 112, 1)\n",
      "230\n",
      "(112, 112, 1)\n",
      "230\n",
      "(112, 112, 1)\n",
      "230\n",
      "(112, 112, 1)\n",
      "282\n",
      "(112, 112, 1)\n",
      "282\n",
      "(112, 112, 1)\n",
      "282\n",
      "(112, 112, 1)\n",
      "170\n",
      "(112, 112, 1)\n",
      "170\n",
      "(112, 112, 1)\n",
      "170\n",
      "(112, 112, 1)\n",
      "146\n",
      "(112, 112, 1)\n",
      "146\n",
      "(112, 112, 1)\n",
      "146\n",
      "(112, 112, 1)\n",
      "158\n",
      "(112, 112, 1)\n",
      "158\n",
      "(112, 112, 1)\n",
      "158\n",
      "(112, 112, 1)\n",
      "223\n",
      "(112, 112, 1)\n",
      "223\n",
      "(112, 112, 1)\n",
      "223\n",
      "(112, 112, 1)\n",
      "377\n",
      "(112, 112, 1)\n",
      "377\n",
      "(112, 112, 1)\n",
      "377\n",
      "(112, 112, 1)\n",
      "241\n",
      "(112, 112, 1)\n",
      "241\n",
      "(112, 112, 1)\n",
      "241\n",
      "(112, 112, 1)\n",
      "614\n",
      "(112, 112, 1)\n",
      "614\n",
      "(112, 112, 1)\n",
      "614\n",
      "(112, 112, 1)\n",
      "467\n",
      "(112, 112, 1)\n",
      "467\n",
      "(112, 112, 1)\n",
      "467\n",
      "(112, 112, 1)\n",
      "280\n",
      "(112, 112, 1)\n",
      "280\n",
      "(112, 112, 1)\n",
      "280\n",
      "(112, 112, 1)\n",
      "508\n",
      "(112, 112, 1)\n",
      "508\n",
      "(112, 112, 1)\n",
      "508\n",
      "(112, 112, 1)\n",
      "333\n",
      "(112, 112, 1)\n",
      "333\n",
      "(112, 112, 1)\n",
      "333\n",
      "(112, 112, 1)\n",
      "357\n",
      "(112, 112, 1)\n",
      "357\n",
      "(112, 112, 1)\n",
      "357\n",
      "(112, 112, 1)\n",
      "464\n",
      "(112, 112, 1)\n",
      "464\n",
      "(112, 112, 1)\n",
      "464\n",
      "(112, 112, 1)\n",
      "561\n",
      "(112, 112, 1)\n",
      "561\n",
      "(112, 112, 1)\n",
      "561\n",
      "(112, 112, 1)\n",
      "185\n",
      "(112, 112, 1)\n",
      "185\n",
      "(112, 112, 1)\n",
      "185\n",
      "(112, 112, 1)\n",
      "287\n",
      "(112, 112, 1)\n",
      "287\n",
      "(112, 112, 1)\n",
      "287\n",
      "(112, 112, 1)\n",
      "366\n",
      "(112, 112, 1)\n",
      "366\n",
      "(112, 112, 1)\n",
      "366\n",
      "(112, 112, 1)\n",
      "148\n",
      "(112, 112, 1)\n",
      "148\n",
      "(112, 112, 1)\n",
      "148\n",
      "(112, 112, 1)\n",
      "172\n",
      "(112, 112, 1)\n",
      "172\n",
      "(112, 112, 1)\n",
      "172\n",
      "(112, 112, 1)\n",
      "294\n",
      "(112, 112, 1)\n",
      "294\n",
      "(112, 112, 1)\n",
      "294\n",
      "(112, 112, 1)\n",
      "225\n",
      "(112, 112, 1)\n",
      "225\n",
      "(112, 112, 1)\n",
      "225\n",
      "(112, 112, 1)\n",
      "215\n",
      "(112, 112, 1)\n",
      "215\n",
      "(112, 112, 1)\n",
      "215\n",
      "(112, 112, 1)\n",
      "220\n",
      "(112, 112, 1)\n",
      "220\n",
      "(112, 112, 1)\n",
      "220\n",
      "(112, 112, 1)\n",
      "250\n",
      "(112, 112, 1)\n",
      "250\n",
      "(112, 112, 1)\n",
      "250\n",
      "(112, 112, 1)\n",
      "148\n",
      "(112, 112, 1)\n",
      "148\n",
      "(112, 112, 1)\n",
      "148\n",
      "(112, 112, 1)\n",
      "140\n",
      "(112, 112, 1)\n",
      "140\n",
      "(112, 112, 1)\n",
      "140\n",
      "(112, 112, 1)\n",
      "239\n",
      "(112, 112, 1)\n",
      "239\n",
      "(112, 112, 1)\n",
      "239\n",
      "(112, 112, 1)\n",
      "216\n",
      "(112, 112, 1)\n",
      "216\n",
      "(112, 112, 1)\n",
      "216\n",
      "(112, 112, 1)\n",
      "267\n",
      "(112, 112, 1)\n",
      "267\n",
      "(112, 112, 1)\n",
      "267\n",
      "(112, 112, 1)\n",
      "161\n",
      "(112, 112, 1)\n",
      "161\n",
      "(112, 112, 1)\n",
      "161\n",
      "(112, 112, 1)\n",
      "Loaded 62 videos.\n",
      "Unique labels: {'tense', 'repression', 'happiness', 'surprise', 'disgust'}\n"
     ]
    }
   ],
   "source": [
    "video_data, video_labels = load_videos(base_dataset_path, data)\n",
    "\n",
    "# Check the loaded video data and labels\n",
    "print(f\"Loaded {len(video_data)} videos.\")\n",
    "print(f\"Unique labels: {set(video_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b63a4c8-b55a-4383-9430-fa28f3913a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Data\n",
    "def preprocess_data(X, y):\n",
    "    # Convert list of images to numpy array\n",
    "    X = np.array(X, dtype='float32')\n",
    "    \n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    X = X / 255.0\n",
    "    \n",
    "    # Resize images if needed (for example, to 112 x 112)\n",
    "    output_size = (112, 112)  # Example target size\n",
    "    X = X.reshape(-1, output_size[0], output_size[1], 3)  # 3 channels for RGB\n",
    "    \n",
    "    # Encode labels\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(y)  # Transform the labels into numeric values\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1c6949e-e697-4025-9909-b8080875c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LearNet_Modelbuild(height=112, width=112, channels=3, classes=8):\n",
    "    im = Input(shape=(height, width, channels))\n",
    "    Conv_S = Conv2D(16, (3, 3), activation='relu', padding='same', strides=2, name='Conv_S')(im)\n",
    "\n",
    "    Conv_1_1 = Conv2D(16, (1, 1), activation='relu', padding='same', strides=2, name='Conv_1_1')(Conv_S)\n",
    "    Conv_1_2 = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2, name='Conv_1_2')(Conv_1_1)\n",
    "    Conv_1_3 = Conv2D(64, (5, 5), activation='relu', padding='same', strides=2, name='Conv_1_3')(Conv_1_2)\n",
    "\n",
    "    Conv_2_1 = Conv2D(16, (1, 1), activation='relu', padding='same', strides=2, name='Conv_2_1')(Conv_S)\n",
    "    add_2_1 = add([Conv_1_1, Conv_2_1])\n",
    "    batch_r11 = BatchNormalization()(add_2_1)\n",
    "    Conv_2_2 = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2, name='Conv_2_2')(batch_r11)\n",
    "    add_2_2 = add([Conv_1_2, Conv_2_2])\n",
    "    batch_r12 = BatchNormalization()(add_2_2)\n",
    "    Conv_x_2 = Conv2D(64, (5, 5), activation='relu', padding='same', strides=2, name='Conv_x_2')(batch_r12)\n",
    "\n",
    "    Conv_3_1 = Conv2D(16, (1, 1), activation='relu', padding='same', strides=2, name='Conv_3_1')(Conv_S)\n",
    "    Conv_3_2 = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2, name='Conv_3_2')(Conv_3_1)\n",
    "    Conv_3_3 = Conv2D(64, (5, 5), activation='relu', padding='same', strides=2, name='Conv_3_3')(Conv_3_2)\n",
    "\n",
    "    Conv_4_1 = Conv2D(16, (1, 1), activation='relu', padding='same', strides=2, name='Conv_4_1')(Conv_S)\n",
    "    add_4_1 = add([Conv_3_1, Conv_4_1])\n",
    "    batch_r13 = BatchNormalization()(add_4_1)\n",
    "    Conv_4_2 = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2, name='Conv_4_2')(batch_r13)\n",
    "    add_4_2 = add([Conv_3_2, Conv_4_2])\n",
    "    batch_r14 = BatchNormalization()(add_4_2)\n",
    "    Conv_x_4 = Conv2D(64, (5, 5), activation='relu', padding='same', strides=2, name='Conv_x_4')(batch_r14)\n",
    "\n",
    "    concta1 = concatenate([Conv_1_3, Conv_x_2, Conv_3_3, Conv_x_4])\n",
    "    batch_X = BatchNormalization()(concta1)\n",
    "\n",
    "    Conv_5_1 = Conv2D(256, (3, 3), activation='relu', padding='same', strides=2, name='Conv_5_1')(batch_X)\n",
    "\n",
    "    F1 = Flatten()(Conv_5_1)\n",
    "    FC1 = Dense(256, activation='relu')(F1)\n",
    "    drop = Dropout(0.5)(FC1)\n",
    "\n",
    "    out = Dense(classes, activation='relu')(drop)\n",
    "\n",
    "    model = Model(inputs=[im], outputs=out)\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a736f2dd-cb41-4fa1-8d5e-8107a50568b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LearNet model\n",
    "lear_net = LearNet_Modelbuild()\n",
    "\n",
    "# Extract intermediate features (output from \"Conv_5_1\")\n",
    "feature_extractor = Model(inputs=lear_net.input, outputs=lear_net.get_layer('Conv_5_1').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03a4fac8-1641-4daa-a96f-33d310ab6ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_features(image_path, feature_extractor):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (112, 112)) / 255.0  # Resize and normalize\n",
    "    image = np.expand_dims(image, axis=0)          # Add batch dimension\n",
    "    features = feature_extractor.predict(image)   # Extract features\n",
    "    return features.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f475e85-17cc-4beb-9683-319f566df733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video_features(video_path, feature_extractor):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    features = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (112, 112)) / 255.0  # Resize and normalize\n",
    "        frame = np.expand_dims(frame, axis=0)          # Add batch dimension\n",
    "        frame_features = feature_extractor.predict(frame)\n",
    "        features.append(frame_features.flatten())      # Store flattened features\n",
    "    cap.release()\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "115d1e2a-c8bb-4b87-a9e1-dfc6308d3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dataset_path = \"data\"\n",
    "# excel_file_path = \"data/Section A.xls\"\n",
    "# data = pd.read_excel(excel_file_path)\n",
    "# image_data, image_labels = load_images_from_EP_folders(base_dataset_path, data)\n",
    "# video_frames, video_labels = load_videos_from_subfolders(base_dataset_path, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fc1a1f6-c867-474a-b73e-87d7769c0339",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocess Data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_images, y_images \u001b[38;5;241m=\u001b[39m preprocess_data(image_data, image_labels)\n\u001b[0;32m      3\u001b[0m X_videos, y_videos \u001b[38;5;241m=\u001b[39m preprocess_data(video_data, video_labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Preprocess Data\n",
    "X_images, y_images = preprocess_data(image_data, image_labels)\n",
    "X_videos, y_videos = preprocess_data(video_data, video_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a41a3168-5f72-4b69-bd53-b5e39f527ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine image and video data\n",
    "X = np.concatenate([X_images, X_videos])\n",
    "y = np.concatenate([y_images, y_videos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c120d090-ff33-4cd4-9d12-3f20bea4235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2f0a7b7-3606-4f49-99ac-c5c7f8bc24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure labels are one-hot encoded\n",
    "num_classes = len(set(y))  # Dynamically infer number of classes\n",
    "y_train_onehot = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_onehot = to_categorical(y_test, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa9bdfc4-18a1-4395-b031-22ef84f0662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LearNet_Modelbuild(height=112,width=112,channels=3,classes =8):\n",
    "\n",
    "    im =Input(shape=(112,112,3))\n",
    "    Conv_S=Conv2D(16, (3,3), activation='relu', padding='same', strides=2, name='Conv_S')(im)\n",
    "\n",
    "\n",
    "    Conv_1_1=Conv2D(16, (1,1), activation='relu', padding='same', strides=2, name='Conv_1_1')(Conv_S)\n",
    "    Conv_1_2=Conv2D(32, (3,3), activation='relu', padding='same', strides=2, name='Conv_1_2')(Conv_1_1)\n",
    "    Conv_1_3=Conv2D(64, (5,5), activation='relu', padding='same', strides=2, name='Conv_1_3')(Conv_1_2)\n",
    "\n",
    "\n",
    "    Conv_2_1=Conv2D(16, (1,1), activation='relu', padding='same', strides=2, name='Conv_2_1')(Conv_S)\n",
    "    add_2_1=add([Conv_1_1, Conv_2_1])\n",
    "    batch_r11=BatchNormalization()(add_2_1)\n",
    "    Conv_2_2=Conv2D(32, (3,3), activation='relu', padding='same', strides=2, name='Conv_2_2')(batch_r11)\n",
    "    add_2_2=add([Conv_1_2, Conv_2_2])\n",
    "    batch_r12=BatchNormalization()(add_2_2)\n",
    "    Conv_x_2=Conv2D(64, (5,5), activation='relu', padding='same', strides=2, name='Conv_x_2')(batch_r12)\n",
    "\n",
    "\n",
    "    Conv_3_1=Conv2D(16, (1,1), activation='relu', padding='same', strides=2, name='Conv_3_1')(Conv_S)\n",
    "    Conv_3_2=Conv2D(32, (3,3), activation='relu', padding='same', strides=2, name='Conv_3_2')(Conv_3_1)\n",
    "    Conv_3_3=Conv2D(64, (5,5), activation='relu', padding='same', strides=2, name='Conv_3_3')(Conv_3_2)\n",
    "\n",
    "\n",
    "    Conv_4_1=Conv2D(16, (1,1), activation='relu', padding='same', strides=2, name='Conv_4_1')(Conv_S)\n",
    "    add_4_1=add([Conv_3_1, Conv_4_1])\n",
    "    batch_r13=BatchNormalization()(add_4_1)\n",
    "    Conv_4_2=Conv2D(32, (3,3), activation='relu', padding='same', strides=2, name='Conv_4_2')(batch_r13)\n",
    "    add_4_2=add([Conv_3_2, Conv_4_2])\n",
    "    batch_r14=BatchNormalization()(add_4_2)\n",
    "    Conv_x_4=Conv2D(64, (5,5), activation='relu', padding='same', strides=2, name='Conv_x_4')(batch_r14)\n",
    "\n",
    "\n",
    "    concta1=concatenate([Conv_1_3, Conv_x_2, Conv_3_3, Conv_x_4])\n",
    "    batch_X=BatchNormalization()(concta1)\n",
    "\n",
    "\n",
    "    Conv_5_1=Conv2D(256, (3,3), activation='relu', padding='same', strides=2, name='Conv_5_1')(batch_X)\n",
    "\n",
    "    F1=Flatten()(Conv_5_1)\n",
    "    FC1=Dense(256,activation='relu')(F1)\n",
    "    drop=Dropout(0.5)(FC1)\n",
    "\n",
    "    out = Dense(classes, activation='relu')(drop)\n",
    "\n",
    "    model = Model(inputs=[im],outputs= out)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88012f86-ca8a-45b6-a8ad-8d37010721f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LearNet model\n",
    "lear_net = LearNet_Modelbuild()\n",
    "\n",
    "# Extract intermediate features (output from \"Conv_5_1\")\n",
    "feature_extractor = Model(inputs=lear_net.input, outputs=lear_net.get_layer('Conv_5_1').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "288f4da4-7c1e-42e5-a381-9fadf069f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the input data\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24d0c2b4-9fdf-4c78-93ee-65c0205d96be",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30f267ca-66c1-4905-b089-206db039a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Checkpoint Callback\n",
    "checkpoint = ModelCheckpoint('lear_net.keras', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f97bd1e-ad3b-4bd2-bda8-366905d88c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['keras_tensor_29']\n",
      "Received: inputs=Tensor(shape=(None, 112, 112, 3))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step - accuracy: 0.4593 - loss: 2.1602"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['keras_tensor_29']\n",
      "Received: inputs=Tensor(shape=(32, 112, 112, 3))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.24444, saving model to lear_net.keras\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 305ms/step - accuracy: 0.4594 - loss: 2.1599 - val_accuracy: 0.2444 - val_loss: 3.7298\n",
      "Epoch 2/10\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.4882 - loss: 1.5767\n",
      "Epoch 2: val_accuracy improved from 0.24444 to 0.27194, saving model to lear_net.keras\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 114ms/step - accuracy: 0.4882 - loss: 1.5767 - val_accuracy: 0.2719 - val_loss: 3.9134\n",
      "Epoch 3/10\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.4953 - loss: 1.5466\n",
      "Epoch 3: val_accuracy did not improve from 0.27194\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 112ms/step - accuracy: 0.4953 - loss: 1.5466 - val_accuracy: 0.2444 - val_loss: 2.7410\n",
      "Epoch 4/10\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.4980 - loss: 1.5526\n",
      "Epoch 4: val_accuracy did not improve from 0.27194\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 110ms/step - accuracy: 0.4980 - loss: 1.5525 - val_accuracy: 0.2444 - val_loss: 3.6133\n",
      "Epoch 5/10\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.5095 - loss: 1.4849\n",
      "Epoch 5: val_accuracy improved from 0.27194 to 0.47055, saving model to lear_net.keras\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 111ms/step - accuracy: 0.5095 - loss: 1.4848 - val_accuracy: 0.4706 - val_loss: 1.3862\n",
      "Epoch 6/10\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.5090 - loss: 1.4344\n",
      "Epoch 6: val_accuracy did not improve from 0.47055\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 114ms/step - accuracy: 0.5090 - loss: 1.4345 - val_accuracy: 0.0335 - val_loss: 5.9208\n",
      "Epoch 7/10\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.5049 - loss: 1.4710\n",
      "Epoch 7: val_accuracy improved from 0.47055 to 0.47686, saving model to lear_net.keras\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 111ms/step - accuracy: 0.5049 - loss: 1.4710 - val_accuracy: 0.4769 - val_loss: 1.5226\n",
      "Epoch 8/10\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.5137 - loss: 1.3960\n",
      "Epoch 8: val_accuracy improved from 0.47686 to 0.52088, saving model to lear_net.keras\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 112ms/step - accuracy: 0.5137 - loss: 1.3960 - val_accuracy: 0.5209 - val_loss: 2.1052\n",
      "Epoch 9/10\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.5135 - loss: 1.4344\n",
      "Epoch 9: val_accuracy did not improve from 0.52088\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 114ms/step - accuracy: 0.5135 - loss: 1.4344 - val_accuracy: 0.4736 - val_loss: 2.4434\n",
      "Epoch 10/10\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.5078 - loss: 1.4192\n",
      "Epoch 10: val_accuracy did not improve from 0.52088\n",
      "\u001b[1m832/832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 113ms/step - accuracy: 0.5078 - loss: 1.4191 - val_accuracy: 0.4127 - val_loss: 2.7138\n"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "history = lear_net.fit(datagen.flow(X_train, y_train_onehot, batch_size=32),\n",
    "                       epochs=10,\n",
    "                       validation_data=(X_test, y_test_onehot),\n",
    "                       callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "318335c8-a381-4ca4-b85e-1c8754c4381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion labels (these are the 8 emotions you're interested in)\n",
    "emotion_labels = ['tense', 'happiness', 'repression', 'disgust', 'surprise', 'contempt', 'fear', 'sadness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db830282-d42b-4098-97af-1a393dc61a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the model\n",
    "def save_model(model, filename='lear_net.h5'):\n",
    "    model.save(filename)\n",
    "    print(f\"Model saved as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cc69ba8-4a86-488d-a484-9334a95ae390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model (after training)\n",
    "def load_trained_model(filename='lear_net.h5'):\n",
    "    model = load_model(filename)\n",
    "    print(f\"Model loaded from {filename}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "510cd0c2-1788-454a-9d4e-ba0e5f39d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test loss: {loss}\")\n",
    "    print(f\"Test accuracy: {accuracy}\")\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16b7e86f-ebfa-454c-b289-920ae9ede16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the micro facial expression for a single image\n",
    "def predict_expression(image_path, model, target_size=(112, 112)):\n",
    "    # Load and preprocess the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, target_size)  # Resize to match model input\n",
    "    image = img_to_array(image) / 255.0  # Normalize the image\n",
    "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Predict using the model\n",
    "    prediction = model.predict(image)\n",
    "\n",
    "    # Get the predicted label (assuming the labels are one-hot encoded)\n",
    "    predicted_label = np.argmax(prediction, axis=1)[0]\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63add652-db30-4970-9e3e-231d0f8be8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping Numerical Prediction to Emotion:\n",
    "def get_emotion_label(predicted_label, label_encoder):\n",
    "    emotion = label_encoder.classes_[predicted_label]\n",
    "    return emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0172a817-83e8-4407-80fa-c9f75bd70bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Prediction on a Single Image:\n",
    "def test_single_prediction(image_path, model, label_encoder, target_size=(112, 112)):\n",
    "    predicted_label = predict_expression(image_path, model, target_size)\n",
    "    emotion = get_emotion_label(predicted_label, label_encoder)\n",
    "    print(f\"Predicted Micro Expression: {emotion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c4dee7a-69f0-4e62-839e-1dcdc5e57889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LabelEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LabelEncoder()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(emotion_labels)  # Fit on the 8 emotion labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed78b3b8-5176-45f8-829a-e3abb5d73130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as lear_net.h5\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m save_model(lear_net)\n\u001b[1;32m----> 3\u001b[0m evaluate_model(lear_net, X_test, y_test_onehot)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "save_model(lear_net)\n",
    "\n",
    "evaluate_model(lear_net, X_test, y_test_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7496b1ac-16ff-4feb-8785-0443d1a09fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "image_path =\"E:/FacialMicroExpression/data/sub01/EP01_5/EP01_5-1.jpg\"\n",
    "# Load the image\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "if image is None:\n",
    "    print(f\"Error: Image not found or unable to load at {image_path}\")\n",
    "else:\n",
    "    print(\"Image loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58727c07-f0db-4f90-ad6a-7e2742afdc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted Micro Expression: disgust\n"
     ]
    }
   ],
   "source": [
    "#Test on a Single Image:\n",
    "image_path = \"data/sub01/EP01_5/EP01_5-1.jpg\"\n",
    "test_single_prediction(image_path, lear_net, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bce67f8-3e4a-4e1f-9721-58946f138d22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
