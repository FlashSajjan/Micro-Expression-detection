{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c3b2a2-3bbe-405f-b5a3-015fbe5f32a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Flatten, Dense, Dropout, concatenate, add\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774728aa-6b26-45bd-b104-74418969bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenCV Haar Cascade classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012322a-eb2c-4a70-abba-cee50b06f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Preprocessing from Excel\n",
    "\n",
    "base_dataset_path = \"E:/FacialMicroExpression/data\"\n",
    "excel_file_path = \"Section A.xls\"\n",
    "output_size = (112, 112)\n",
    "motion_threshold = 1e-3\n",
    "\n",
    "# Read the Excel file to get the emotion labels for each sub folder\n",
    "def load_data_from_excel(excel_file_path):\n",
    "    \"\"\"Reads the Excel file containing filenames and labels.\"\"\"\n",
    "    data = pd.read_excel(excel_file_path)\n",
    "    return data\n",
    "print(\"Excel columns:\", load_data_from_excel(excel_file_path).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb553615-d74e-4ddd-88ad-0d00e2873d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and normalization\n",
    "# Path to the Excel file\n",
    "excel_file_path = \"Section A.xls\"\n",
    "\n",
    "# Load the Excel file\n",
    "data = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Display the original DataFrame for reference\n",
    "print(\"Original Data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Data cleaning and normalization\n",
    "# Normalize columns: Subject, Filename, and Emotion\n",
    "data['Subject'] = data['Subject'].astype(str).str.strip().str.lower()  # Normalize Subject column\n",
    "data['Filename'] = data['Filename'].astype(str).str.strip().str.lower()  # Normalize Filename column\n",
    "data['Emotion'] = data['Emotion'].astype(str).str.strip().str.lower()  # Normalize Emotion column\n",
    "\n",
    "# Handle missing values in critical columns\n",
    "data.dropna(subset=['Subject', 'Filename', 'Emotion'], inplace=True)  # Drop rows with missing values in these columns\n",
    "\n",
    "# Remove duplicates if any (considering all columns for uniqueness)\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(\"\\nCleaned Data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Display unique emotions for verification\n",
    "print(\"\\nUnique Emotions:\")\n",
    "print(data['Emotion'].unique())\n",
    "\n",
    "# Display the count of unique subjects and filenames for debugging\n",
    "print(\"\\nUnique Subjects:\")\n",
    "print(data['Subject'].nunique())\n",
    "print(\"\\nUnique Filenames:\")\n",
    "print(data['Filename'].nunique())\n",
    "data = data.drop(columns=['Unnamed: 2', 'Unnamed: 7'], errors='ignore')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b385fa85-f74c-4b4a-ac4c-63a64cc0ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_EP_folders(base_dataset_path, data):\n",
    "    image_data = []\n",
    "    image_labels = []\n",
    "    unmatched_folders = []\n",
    "\n",
    "    for subfolder in os.listdir(base_dataset_path):\n",
    "        subfolder_path = os.path.join(base_dataset_path, subfolder)\n",
    "\n",
    "        # Format the subject (e.g., 1 -> sub01, 2 -> sub02)\n",
    "        subject_id = subfolder.strip().lower().replace(\"sub\", \"\")  # Remove 'sub' and match\n",
    "        matching_subjects = data[data['Subject'].astype(str).str.zfill(2) == subject_id]\n",
    "\n",
    "        if not matching_subjects.empty:\n",
    "            for ep_folder in os.listdir(subfolder_path):\n",
    "                ep_folder_path = os.path.join(subfolder_path, ep_folder)\n",
    "\n",
    "                # Match ep_folder with Filename column in Excel (normalize case)\n",
    "                matching_filenames = matching_subjects[matching_subjects['Filename'].str.strip().str.lower() == ep_folder.strip().lower()]\n",
    "\n",
    "                if not matching_filenames.empty and os.path.isdir(ep_folder_path):\n",
    "                    for file_name in os.listdir(ep_folder_path):\n",
    "                        if file_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                            image_path = os.path.join(ep_folder_path, file_name)\n",
    "\n",
    "                            image = cv2.imread(image_path)\n",
    "                            if image is not None:\n",
    "                                resized_image = cv2.resize(image, (112, 112))\n",
    "                                image_data.append(resized_image)\n",
    "\n",
    "                                # Extract emotion label from matching row in Excel\n",
    "                                label = matching_filenames['Emotion'].values[0]\n",
    "                                image_labels.append(label)\n",
    "                else:\n",
    "                    unmatched_folders.append(ep_folder)\n",
    "        else:\n",
    "            unmatched_folders.append(subfolder)\n",
    "\n",
    "    print(f\"Loaded {len(image_data)} images.\")\n",
    "    print(f\"Loaded {len(image_labels)} labels.\")\n",
    "    print(f\"Unique labels: {set(image_labels)}\")\n",
    "    print(f\"Unmatched folders: {set(unmatched_folders)}\")\n",
    "\n",
    "    return image_data, image_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a7234a-e470-4c24-be0d-4b1c71801322",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data, image_labels = load_images_from_EP_folders(base_dataset_path, data)\n",
    "for idx, image in enumerate(image_data[:5]):  # Check first 5 images\n",
    "    print(f\"Image {idx + 1} shape: {image.shape}\")\n",
    "unique_labels = data['Emotion'].unique()\n",
    "print(f\"Unique labels in the dataset: {unique_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a2334-4cc9-4a0e-b704-23b8ac85df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (image, label) in enumerate(zip(image_data[:5], image_labels[:5])):  # First 5 pairs\n",
    "    print(f\"Image {i + 1}: Label = {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97366dea-1467-4a91-a9ec-c616fd994ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dynamic_image(frames, normalized=True):\n",
    "    \"\"\" Takes a list of frames and returns either a raw or normalized dynamic image.\"\"\"\n",
    "    num_channels = frames[0].shape[2]\n",
    "    channel_frames = _get_channel_frames(frames, num_channels)\n",
    "    channel_dynamic_images = [_compute_dynamic_image(channel) for channel in channel_frames]\n",
    "\n",
    "    dynamic_image = cv2.merge(tuple(channel_dynamic_images))\n",
    "    if normalized:\n",
    "        dynamic_image = cv2.normalize(dynamic_image, None, 0, 255, norm_type=cv2.NORM_MINMAX)\n",
    "        dynamic_image = dynamic_image.astype('uint8')\n",
    "\n",
    "    return dynamic_image\n",
    "\n",
    "\n",
    "def _get_channel_frames(iter_frames, num_channels):\n",
    "    \"\"\" Takes a list of frames and returns a list of frame lists split by channel. \"\"\"\n",
    "    frames = [[] for channel in range(num_channels)]\n",
    "\n",
    "    for frame in iter_frames:\n",
    "        for channel_frames, channel in zip(frames, cv2.split(frame)):\n",
    "            channel_frames.append(channel.reshape((*channel.shape[0:2], 1)))\n",
    "    for i in range(len(frames)):\n",
    "        frames[i] = np.array(frames[i])\n",
    "    return frames\n",
    "\n",
    "\n",
    "def _compute_dynamic_image(frames):\n",
    "    num_frames, h, w, depth = frames.shape\n",
    "\n",
    "    y = np.zeros((num_frames, h, w, depth))\n",
    "\n",
    "    ids = np.ones(num_frames)\n",
    "\n",
    "    fw = np.zeros(num_frames)\n",
    "    for n in range(num_frames):\n",
    "        cumulative_indices = np.array(range(n, num_frames)) + 1\n",
    "        fw[n] = np.sum(((2*cumulative_indices) - num_frames) / cumulative_indices)\n",
    "\n",
    "    for v in range(int(np.max(ids))):\n",
    "        indv = np.array(np.where(ids == v+1))\n",
    "\n",
    "        a1 = frames[indv, :, :, :]\n",
    "        a2 = np.reshape(fw, (indv.shape[1], 1, 1, 1))\n",
    "        a3 = a1 * a2\n",
    "        print(indv.shape[1])\n",
    "\n",
    "        y = np.sum(a3[0], axis=0)\n",
    "        print(y.shape)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d2375-d2b7-4628-808f-fae0e5e13647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_videos(base_dataset_path, data, output_size=(112, 112)):\n",
    "    video_data = []\n",
    "    video_labels = []\n",
    "\n",
    "    # Iterate through subject folders (sub01, sub02, ...)\n",
    "    for subject_folder in os.listdir(base_dataset_path):\n",
    "        subject_path = os.path.join(base_dataset_path, subject_folder)\n",
    "\n",
    "        # Check if it's a valid directory (e.g., sub01, sub02)\n",
    "        if os.path.isdir(subject_path):\n",
    "            # Match subject with Excel data (e.g., sub01 with Subject 1)\n",
    "            matching_subjects = data[\n",
    "                data['Subject'].astype(str).str.strip().str.lower() == subject_folder.strip().lower().replace('sub', '')\n",
    "            ]\n",
    "            label = matching_subjects['Emotion'].values[0] if not matching_subjects.empty else None\n",
    "\n",
    "            # Process video files inside the subject folder\n",
    "            for file_name in os.listdir(subject_path):\n",
    "                if file_name.endswith('.avi'):  # Look for .avi video files\n",
    "                    video_filename = file_name.split('.')[0]  # Get EPXXXX part of the filename (e.g., EP01)\n",
    "\n",
    "                    # Match video filename with Excel 'Filename' column\n",
    "                    matching_video = matching_subjects[matching_subjects['Filename'].str.strip().str.lower() == video_filename.strip().lower()]\n",
    "                    if not matching_video.empty:\n",
    "                        video_path = os.path.join(subject_path, file_name)\n",
    "                        cap = cv2.VideoCapture(video_path)\n",
    "                        frames = []\n",
    "\n",
    "                        # Extract frames from the video\n",
    "                        while cap.isOpened():\n",
    "                            ret, frame = cap.read()\n",
    "                            if not ret:\n",
    "                                break\n",
    "                            resized_frame = cv2.resize(frame, output_size)  # Resize the frame\n",
    "                            frames.append(resized_frame)\n",
    "                        cap.release()\n",
    "\n",
    "                        # Generate dynamic image from frames (optional)\n",
    "                        if len(frames) > 0:\n",
    "                            dynamic_image = get_dynamic_image(frames)\n",
    "                            video_data.append(dynamic_image)\n",
    "                            video_labels.append(label)\n",
    "\n",
    "    return video_data, video_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d87505-1326-43f6-b3c9-e1c6f983d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_data, video_labels = load_videos(base_dataset_path, data)\n",
    "\n",
    "# Check the loaded video data and labels\n",
    "print(f\"Loaded {len(video_data)} videos.\")\n",
    "print(f\"Unique labels: {set(video_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a3a495-15a9-4017-a509-73bd5b7c2e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Data\n",
    "def preprocess_data(X, y):\n",
    "    # Convert list of images to numpy array\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    X = X / 255.0\n",
    "    \n",
    "    # Resize images if needed (for example, to 112 x 112)\n",
    "    output_size = (112, 112)  # Example target size\n",
    "    X = X.reshape(-1, output_size[0], output_size[1], 3)  # 3 channels for RGB\n",
    "    \n",
    "    # Encode labels\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(y)  # Transform the labels into numeric values\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a99c3fc-841a-44d0-8bac-1ad0beabac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LearNet_Modelbuild(height=112, width=112, channels=3, classes=8):\n",
    "    im = Input(shape=(height, width, channels))\n",
    "    Conv_S = Conv2D(16, (3, 3), activation='relu', padding='same', strides=2, name='Conv_S')(im)\n",
    "\n",
    "    Conv_1_1 = Conv2D(16, (1, 1), activation='relu', padding='same', strides=2, name='Conv_1_1')(Conv_S)\n",
    "    Conv_1_2 = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2, name='Conv_1_2')(Conv_1_1)\n",
    "    Conv_1_3 = Conv2D(64, (5, 5), activation='relu', padding='same', strides=2, name='Conv_1_3')(Conv_1_2)\n",
    "\n",
    "    Conv_2_1 = Conv2D(16, (1, 1), activation='relu', padding='same', strides=2, name='Conv_2_1')(Conv_S)\n",
    "    add_2_1 = add([Conv_1_1, Conv_2_1])\n",
    "    batch_r11 = BatchNormalization()(add_2_1)\n",
    "    Conv_2_2 = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2, name='Conv_2_2')(batch_r11)\n",
    "    add_2_2 = add([Conv_1_2, Conv_2_2])\n",
    "    batch_r12 = BatchNormalization()(add_2_2)\n",
    "    Conv_x_2 = Conv2D(64, (5, 5), activation='relu', padding='same', strides=2, name='Conv_x_2')(batch_r12)\n",
    "\n",
    "    Conv_3_1 = Conv2D(16, (1, 1), activation='relu', padding='same', strides=2, name='Conv_3_1')(Conv_S)\n",
    "    Conv_3_2 = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2, name='Conv_3_2')(Conv_3_1)\n",
    "    Conv_3_3 = Conv2D(64, (5, 5), activation='relu', padding='same', strides=2, name='Conv_3_3')(Conv_3_2)\n",
    "\n",
    "    Conv_4_1 = Conv2D(16, (1, 1), activation='relu', padding='same', strides=2, name='Conv_4_1')(Conv_S)\n",
    "    add_4_1 = add([Conv_3_1, Conv_4_1])\n",
    "    batch_r13 = BatchNormalization()(add_4_1)\n",
    "    Conv_4_2 = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2, name='Conv_4_2')(batch_r13)\n",
    "    add_4_2 = add([Conv_3_2, Conv_4_2])\n",
    "    batch_r14 = BatchNormalization()(add_4_2)\n",
    "    Conv_x_4 = Conv2D(64, (5, 5), activation='relu', padding='same', strides=2, name='Conv_x_4')(batch_r14)\n",
    "\n",
    "    concta1 = concatenate([Conv_1_3, Conv_x_2, Conv_3_3, Conv_x_4])\n",
    "    batch_X = BatchNormalization()(concta1)\n",
    "\n",
    "    Conv_5_1 = Conv2D(256, (3, 3), activation='relu', padding='same', strides=2, name='Conv_5_1')(batch_X)\n",
    "\n",
    "    F1 = Flatten()(Conv_5_1)\n",
    "    FC1 = Dense(256, activation='relu')(F1)\n",
    "    drop = Dropout(0.5)(FC1)\n",
    "\n",
    "    out = Dense(classes, activation='relu')(drop)\n",
    "\n",
    "    model = Model(inputs=[im], outputs=out)\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb4cf7-7225-47b8-a325-85878dc6af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LearNet model\n",
    "lear_net = LearNet_Modelbuild()\n",
    "\n",
    "# Extract intermediate features (output from \"Conv_5_1\")\n",
    "feature_extractor = Model(inputs=lear_net.input, outputs=lear_net.get_layer('Conv_5_1').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd5f601-ae7b-4b0c-841b-b5a8dd86a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_features(image_path, feature_extractor):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (112, 112)) / 255.0  # Resize and normalize\n",
    "    image = np.expand_dims(image, axis=0)          # Add batch dimension\n",
    "    features = feature_extractor.predict(image)   # Extract features\n",
    "    return features.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba998ed-1c4c-40d6-83c8-0b7b92f2f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video_features(video_path, feature_extractor):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    features = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (112, 112)) / 255.0  # Resize and normalize\n",
    "        frame = np.expand_dims(frame, axis=0)          # Add batch dimension\n",
    "        frame_features = feature_extractor.predict(frame)\n",
    "        features.append(frame_features.flatten())      # Store flattened features\n",
    "    cap.release()\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc9b805-6442-4e75-bfec-d243c3b847db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Data\n",
    "X_images, y_images = preprocess_data(image_data, image_labels)\n",
    "X_videos, y_videos = preprocess_data(video_data, video_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d81a1a3-b596-4381-96b0-512bf0572d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine image and video data\n",
    "X = np.concatenate([X_images, X_videos])\n",
    "y = np.concatenate([y_images, y_videos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cab9a0-f387-4539-badb-a895de7d53e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
