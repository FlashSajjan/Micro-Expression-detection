{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83419bcc-5981-40e1-94b4-50bd5f73c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Flatten, Dense, Dropout, concatenate, add\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths\n",
    "DATASET_PATH = \"E:/FacialMicroExpression/data\"\n",
    "EXCEL_PATH = \"Section A.xls\"\n",
    "OUTPUT_SIZE = (112, 112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e1ada2-83b9-48da-910d-0ba9afc611e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess Excel\n",
    "def load_and_clean_excel(excel_path):\n",
    "    data = pd.read_excel(excel_path)\n",
    "    data = data.rename(columns=str.strip).dropna(subset=['Subject', 'Filename', 'Emotion'])\n",
    "    for col in ['Subject', 'Filename', 'Emotion']:\n",
    "        data[col] = data[col].astype(str).str.strip().str.lower()\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    data.drop(columns=['Unnamed: 2', 'Unnamed: 7'], errors='ignore', inplace=True)\n",
    "    return data\n",
    "\n",
    "# Apply lighting normalization\n",
    "def normalize_lighting(image):\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0)\n",
    "    cl = clahe.apply(l)\n",
    "    lab = cv2.merge((cl, a, b))\n",
    "    img_clahe = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    gamma = 1.2\n",
    "    img_gamma = np.power(img_clahe / 255.0, gamma) * 255.0\n",
    "    return np.clip(img_gamma, 0, 255).astype(np.uint8)\n",
    "\n",
    "# Load images and map to Excel\n",
    "def load_images(data, base_path):\n",
    "    images, labels = [], []\n",
    "    for sub in os.listdir(base_path):\n",
    "        sub_path = os.path.join(base_path, sub)\n",
    "        subject_id = sub.lower().replace(\"sub\", \"\")\n",
    "        match = data[data['Subject'].str.zfill(2) == subject_id]\n",
    "        if match.empty: continue\n",
    "\n",
    "        for ep_folder in os.listdir(sub_path):\n",
    "            ep_path = os.path.join(sub_path, ep_folder)\n",
    "            row = match[match['Filename'] == ep_folder.lower()]\n",
    "            if not row.empty and os.path.isdir(ep_path):\n",
    "                label = row['Emotion'].values[0]\n",
    "                for file in os.listdir(ep_path):\n",
    "                    if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        img = cv2.imread(os.path.join(ep_path, file))\n",
    "                        if img is not None:\n",
    "                            img = cv2.resize(img, OUTPUT_SIZE)\n",
    "                            img = normalize_lighting(img)\n",
    "                            images.append(img)\n",
    "                            labels.append(label)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa3837-837e-4e83-9138-6551cf50c85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dynamic image from video frames\n",
    "def get_dynamic_image(frames):\n",
    "    def _split_channels(frames, channels=3):\n",
    "        return [np.array([cv2.split(f)[c].reshape(f.shape[:2] + (1,)) for f in frames]) for c in range(channels)]\n",
    "\n",
    "    def _compute_weighted_sum(frames):\n",
    "        T, H, W, C = frames.shape\n",
    "        fw = np.array([sum((2*np.arange(n, T)+1 - T) / (np.arange(n, T)+1)) for n in range(T)])\n",
    "        return np.sum(frames * fw[:, None, None, None], axis=0)\n",
    "\n",
    "    channels = _split_channels(frames)\n",
    "    dyn_image = cv2.merge([_compute_weighted_sum(c) for c in channels])\n",
    "    dyn_image = cv2.normalize(dyn_image, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    return dyn_image\n",
    "\n",
    "# Load videos and generate dynamic images\n",
    "def load_videos(data, base_path):\n",
    "    videos, labels = [], []\n",
    "    for sub in os.listdir(base_path):\n",
    "        sub_path = os.path.join(base_path, sub)\n",
    "        subject_id = sub.lower().replace(\"sub\", \"\")\n",
    "        match = data[data['Subject'].str.zfill(2) == subject_id]\n",
    "        if match.empty: continue\n",
    "\n",
    "        for file in os.listdir(sub_path):\n",
    "            if file.lower().endswith('.avi'):\n",
    "                ep = file.split('.')[0].lower()\n",
    "                row = match[match['Filename'] == ep]\n",
    "                if row.empty:\n",
    "                    print(f\"[WARN] Unmatched video: {sub}/{file}\")\n",
    "                    continue\n",
    "                cap = cv2.VideoCapture(os.path.join(sub_path, file))\n",
    "                frames = []\n",
    "                while cap.isOpened():\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret: break\n",
    "                    frame = cv2.resize(frame, OUTPUT_SIZE)\n",
    "                    frame = normalize_lighting(frame)\n",
    "                    frames.append(frame)\n",
    "                cap.release()\n",
    "                if frames:\n",
    "                    dyn_image = get_dynamic_image(frames)\n",
    "                    videos.append(dyn_image)\n",
    "                    labels.append(row['Emotion'].values[0])\n",
    "    return videos, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e3b15-57ea-445f-a872-67e1bec037c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "def preprocess(X, y):\n",
    "    X = np.array(X, dtype='float32') / 255.0\n",
    "    X = X.reshape(-1, 112, 112, 3)\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2183514-6030-4b71-bb16-b84dfd57d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LearNet_Modelbuild(height=112, width=112, channels=3, classes=8):\n",
    "    im = Input(shape=(height, width, channels))\n",
    "    Conv_S = Conv2D(16, (3, 3), activation='relu', padding='same', strides=2, name='Conv_S')(im)\n",
    "\n",
    "    Conv_1_1 = Conv2D(16, (1, 1), activation='relu', padding='same', strides=2, name='Conv_1_1')(Conv_S)\n",
    "    Conv_1_2 = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2, name='Conv_1_2')(Conv_1_1)\n",
    "    Conv_1_3 = Conv2D(64, (5, 5), activation='relu', padding='same', strides=2, name='Conv_1_3')(Conv_1_2)\n",
    "\n",
    "    Conv_2_1 = Conv2D(16, (1, 1), activation='relu', padding='same', strides=2, name='Conv_2_1')(Conv_S)\n",
    "    add_2_1 = add([Conv_1_1, Conv_2_1])\n",
    "    batch_r11 = BatchNormalization()(add_2_1)\n",
    "    Conv_2_2 = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2, name='Conv_2_2')(batch_r11)\n",
    "    add_2_2 = add([Conv_1_2, Conv_2_2])\n",
    "    batch_r12 = BatchNormalization()(add_2_2)\n",
    "    Conv_x_2 = Conv2D(64, (5, 5), activation='relu', padding='same', strides=2, name='Conv_x_2')(batch_r12)\n",
    "\n",
    "    Conv_3_1 = Conv2D(16, (1, 1), activation='relu', padding='same', strides=2, name='Conv_3_1')(Conv_S)\n",
    "    Conv_3_2 = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2, name='Conv_3_2')(Conv_3_1)\n",
    "    Conv_3_3 = Conv2D(64, (5, 5), activation='relu', padding='same', strides=2, name='Conv_3_3')(Conv_3_2)\n",
    "\n",
    "    Conv_4_1 = Conv2D(16, (1, 1), activation='relu', padding='same', strides=2, name='Conv_4_1')(Conv_S)\n",
    "    add_4_1 = add([Conv_3_1, Conv_4_1])\n",
    "    batch_r13 = BatchNormalization()(add_4_1)\n",
    "    Conv_4_2 = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2, name='Conv_4_2')(batch_r13)\n",
    "    add_4_2 = add([Conv_3_2, Conv_4_2])\n",
    "    batch_r14 = BatchNormalization()(add_4_2)\n",
    "    Conv_x_4 = Conv2D(64, (5, 5), activation='relu', padding='same', strides=2, name='Conv_x_4')(batch_r14)\n",
    "\n",
    "    concta1 = concatenate([Conv_1_3, Conv_x_2, Conv_3_3, Conv_x_4])\n",
    "    batch_X = BatchNormalization()(concta1)\n",
    "\n",
    "    Conv_5_1 = Conv2D(256, (3, 3), activation='relu', padding='same', strides=2, name='Conv_5_1')(batch_X)\n",
    "\n",
    "    F1 = Flatten()(Conv_5_1)\n",
    "    FC1 = Dense(256, activation='relu')(F1)\n",
    "    drop = Dropout(0.5)(FC1)\n",
    "\n",
    "    out = Dense(classes, activation='softmax')(drop)\n",
    "\n",
    "    model = Model(inputs=[im], outputs=out)\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39151f96-7644-4232-bd40-b9e82fc3f22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Unmatched video: sub01/EP14_8.avi\n",
      "[WARN] Unmatched video: sub07/EP17_1.avi\n",
      "[WARN] Unmatched video: sub07/EP17_2.avi\n",
      "[WARN] Unmatched video: sub07/EP17_3.avi\n",
      "[WARN] Unmatched video: sub07/EP17_7.avi\n"
     ]
    }
   ],
   "source": [
    "df = load_and_clean_excel(EXCEL_PATH)\n",
    "img_data, img_labels = load_images(df, DATASET_PATH)\n",
    "vid_data, vid_labels = load_videos(df, DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9ce730-6196-464b-8551-464498add9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess(img_data + vid_data, img_labels + vid_labels)\n",
    "y_onehot = to_categorical(y, num_classes=len(set(y)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d325ef-22b3-4190-98a0-e16970592246",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LearNet(classes=y_onehot.shape[1])\n",
    "aug = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    horizontal_flip=True,\n",
    "    channel_shift_range=20.0,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81af3110-946c-48cc-b133-68d854b12036",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "        ModelCheckpoint(\"lear_net_final.keras\", save_best_only=True, monitor=\"val_accuracy\", mode=\"max\"),\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=50, restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf096bda-4cd5-4a3d-a22c-fbbb4e76797d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 142ms/step - accuracy: 0.3566 - loss: 1.6442 - val_accuracy: 0.3760 - val_loss: 3983.5811\n",
      "Epoch 2/150\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 141ms/step - accuracy: 0.3895 - loss: 1.5696 - val_accuracy: 0.3760 - val_loss: 5583.7671\n",
      "Epoch 3/150\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 139ms/step - accuracy: 0.3837 - loss: 1.5720 - val_accuracy: 0.3760 - val_loss: 5702.4380\n",
      "Epoch 4/150\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 141ms/step - accuracy: 0.3843 - loss: 1.5621 - val_accuracy: 0.3760 - val_loss: 2785.0361\n",
      "Epoch 5/150\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 137ms/step - accuracy: 0.3895 - loss: 1.5612 - val_accuracy: 0.3760 - val_loss: 2887.7327\n",
      "Epoch 6/150\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 137ms/step - accuracy: 0.3854 - loss: 1.5615 - val_accuracy: 0.3760 - val_loss: 2907.3496\n",
      "Epoch 7/150\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 136ms/step - accuracy: 0.3861 - loss: 1.5578 - val_accuracy: 0.3760 - val_loss: 3365.5928\n",
      "Epoch 8/150\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 136ms/step - accuracy: 0.3869 - loss: 1.5547 - val_accuracy: 0.3760 - val_loss: 6025.5239\n",
      "Epoch 9/150\n",
      "\u001b[1m186/835\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 132ms/step - accuracy: 0.3856 - loss: 1.5600"
     ]
    }
   ],
   "source": [
    "model.fit(aug.flow(X_train, y_train, batch_size=32),\n",
    "              validation_data=(X_test, y_test),\n",
    "              epochs=150,\n",
    "              callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a38010-7e16-42de-a91f-f32386981145",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"lear_net_final.h5\")\n",
    "    print(\"Model saved as lear_net_final.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5497e38-9df3-41e5-ae4d-7a84bed60b36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
