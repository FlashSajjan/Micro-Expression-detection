{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b90bb1b5-bb2c-4887-95e6-db29b1ab8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7934300-ca76-4867-b32d-8e512d753280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Using emotion labels: ['tense', 'happiness', 'repression', 'disgust', 'surprise', 'contempt', 'fear', 'sadness']\n",
      "Starting live emotion detection. Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load the trained LearNet model\n",
    "    model_path = 'lear_net.keras'  # Path to your saved model\n",
    "    model = load_model(model_path)\n",
    "    print(\"Model loaded successfully\")\n",
    "    \n",
    "    # Load face cascade classifier for face detection\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Define emotion labels - IMPORTANT: Update these to match your actual training labels\n",
    "    # These should match the exact labels from your training data\n",
    "    emotion_labels = ['tense', 'happiness', 'repression', 'disgust', 'surprise', 'contempt', 'fear', 'sadness']\n",
    "    print(f\"Using emotion labels: {emotion_labels}\")\n",
    "    \n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam\")\n",
    "        return\n",
    "    \n",
    "    # Frame buffer for dynamic image computation\n",
    "    frame_buffer = []\n",
    "    max_buffer_size = 10  # Adjust based on your model's requirements\n",
    "    \n",
    "    # For FPS calculation\n",
    "    prev_time = time.time()\n",
    "    frame_count = 0\n",
    "    fps = 0\n",
    "    \n",
    "    print(\"Starting live emotion detection. Press 'q' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture frame\")\n",
    "            break\n",
    "            \n",
    "        # Calculate FPS\n",
    "        frame_count += 1\n",
    "        current_time = time.time()\n",
    "        if current_time - prev_time >= 1.0:\n",
    "            fps = frame_count\n",
    "            frame_count = 0\n",
    "            prev_time = current_time\n",
    "        \n",
    "        # Convert to grayscale for face detection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect faces\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "        \n",
    "        # Process each detected face\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Draw rectangle around face\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            \n",
    "            # Extract face ROI\n",
    "            face_roi = frame[y:y+h, x:x+w]\n",
    "            \n",
    "            # Resize to match model input size\n",
    "            try:\n",
    "                face_roi = cv2.resize(face_roi, (112, 112))\n",
    "                \n",
    "                # Add to frame buffer for dynamic image processing\n",
    "                if len(frame_buffer) >= max_buffer_size:\n",
    "                    frame_buffer.pop(0)  # Remove oldest frame\n",
    "                frame_buffer.append(face_roi)\n",
    "                \n",
    "                # Process frame for prediction - using same preprocessing as training\n",
    "                processed_face = face_roi.astype('float32') / 255.0  # Normalize\n",
    "                processed_face = np.expand_dims(processed_face, axis=0)  # Add batch dimension\n",
    "                \n",
    "                # Make prediction\n",
    "                prediction = model.predict(processed_face, verbose=0)\n",
    "                emotion_idx = np.argmax(prediction[0])\n",
    "                \n",
    "                # Ensure index is within bounds\n",
    "                if emotion_idx < len(emotion_labels):\n",
    "                    emotion = emotion_labels[emotion_idx]\n",
    "                    confidence = float(prediction[0][emotion_idx])\n",
    "                    \n",
    "                    # Display emotion and confidence\n",
    "                    label = f\"{emotion}: {confidence:.2f}\"\n",
    "                    cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "                else:\n",
    "                    print(f\"Warning: Predicted index {emotion_idx} out of range for emotion_labels\")\n",
    "                    cv2.putText(frame, \"Unknown emotion\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing face: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Display FPS\n",
    "        cv2.putText(frame, f\"FPS: {fps}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                \n",
    "        # Display the frame\n",
    "        cv2.imshow('Micro-Expression Detection', frame)\n",
    "        \n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Add an option to use dynamic image processing if your model is trained for it\n",
    "def process_with_dynamic_image(frame_buffer, model, emotion_labels):\n",
    "    \"\"\"\n",
    "    Process frames using dynamic image technique and predict emotion\n",
    "    \"\"\"\n",
    "    if len(frame_buffer) < 3:  # Need at least a few frames\n",
    "        return \"Waiting...\", 0.0\n",
    "    \n",
    "    # Generate dynamic image\n",
    "    dynamic_img = get_dynamic_image(frame_buffer)\n",
    "    processed_img = dynamic_img.astype('float32') / 255.0\n",
    "    processed_img = np.expand_dims(processed_img, axis=0)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(processed_img, verbose=0)\n",
    "    emotion_idx = np.argmax(prediction[0])\n",
    "    emotion = emotion_labels[emotion_idx]\n",
    "    confidence = float(prediction[0][emotion_idx])\n",
    "    \n",
    "    return emotion, confidence\n",
    "\n",
    "def get_dynamic_image(frames, normalized=True):\n",
    "    \"\"\" \n",
    "    Takes a list of frames and returns either a raw or normalized dynamic image.\n",
    "    \"\"\"\n",
    "    # Convert to numpy array if not already\n",
    "    frames_array = np.array(frames)\n",
    "    \n",
    "    num_channels = frames_array[0].shape[2]\n",
    "    channel_frames = _get_channel_frames(frames_array, num_channels)\n",
    "    \n",
    "    # Compute dynamic image for each channel\n",
    "    channel_dynamic_images = []\n",
    "    for channel in channel_frames:\n",
    "        try:\n",
    "            dynamic_channel = _compute_dynamic_image(channel)\n",
    "            channel_dynamic_images.append(dynamic_channel)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in dynamic image computation: {str(e)}\")\n",
    "            # If dynamic image computation fails, use the last frame\n",
    "            channel_dynamic_images.append(channel[-1])\n",
    "    \n",
    "    # Merge channels back together\n",
    "    dynamic_image = cv2.merge(tuple(channel_dynamic_images))\n",
    "    \n",
    "    # Normalize if requested\n",
    "    if normalized:\n",
    "        dynamic_image = cv2.normalize(dynamic_image, None, 0, 255, norm_type=cv2.NORM_MINMAX)\n",
    "        dynamic_image = dynamic_image.astype('uint8')\n",
    "    \n",
    "    return dynamic_image\n",
    "\n",
    "def _get_channel_frames(frames, num_channels):\n",
    "    \"\"\" \n",
    "    Takes a list of frames and returns a list of frame lists split by channel.\n",
    "    \"\"\"\n",
    "    channel_frames = [[] for _ in range(num_channels)]\n",
    "    \n",
    "    for frame in frames:\n",
    "        channels = cv2.split(frame)\n",
    "        for i, channel in enumerate(channels):\n",
    "            channel_frames[i].append(channel.reshape((*channel.shape[0:2], 1)))\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    for i in range(len(channel_frames)):\n",
    "        channel_frames[i] = np.array(channel_frames[i])\n",
    "    \n",
    "    return channel_frames\n",
    "\n",
    "def _compute_dynamic_image(frames):\n",
    "    \"\"\"\n",
    "    Compute the dynamic image for a single channel.\n",
    "    \"\"\"\n",
    "    num_frames, h, w, depth = frames.shape\n",
    "    \n",
    "    # Modified implementation to avoid potential shape issues\n",
    "    fw = np.zeros(num_frames)\n",
    "    for n in range(num_frames):\n",
    "        cumulative_indices = np.array(range(n, num_frames))\n",
    "        fw[n] = np.sum(((2*(cumulative_indices+1)) - num_frames) / (cumulative_indices+1))\n",
    "    \n",
    "    # Reshape weights for broadcasting\n",
    "    fw = fw.reshape((-1, 1, 1, 1))\n",
    "    \n",
    "    # Multiply frames by weights and sum\n",
    "    weighted_sum = np.sum(frames * fw, axis=0)\n",
    "    \n",
    "    return weighted_sum\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ede91-4246-45a7-849f-03c5c6792918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
